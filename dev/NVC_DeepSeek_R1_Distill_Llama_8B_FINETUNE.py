# NVC_DeepSeek_R1_Distill_Llama_8B_FINETUNE.py
# -*- coding: utf-8 -*-
"""
This script fine-tunes the DeepSeek model for chain-of-thought (CoT) reasoning
using Nonviolent Communication (NVC) principles, using a dataset generated by NVC_DeepSeek_R1_Distill_Llama_8B_DATA.py.
It leverages Unsloth and the Supervised Fine-Tuning (SFT) Trainer from the TRL library.
"""

from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset, Dataset
import pandas as pd
import torch

# --- 1. Load Model with LoRA Configuration ---
max_seq_length = 2048
lora_rank = 32  # LoRA rank, adjust as needed

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    max_seq_length=max_seq_length,
    load_in_4bit=True, # Keep লোড_ইন_4bit for memory efficiency
    dtype=None, # Auto detect dtype
    max_lora_rank=lora_rank,
    gpu_memory_utilization=0.9, # Adjust based on your GPU memory
)

model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,  # LoRA rank
    target_modules=["q_proj", "v_proj"],  # Target attention layers, you can expand this
    lora_alpha=lora_rank,
    lora_dropout=0.1,
    bias="none",
    use_gradient_checkpointing=True,
    random_state=42,
)

# --- 2. Load and Format the NVC CoT Dataset ---
csv_file_path = "nvc_cot_dataset.csv"  # Path to your generated dataset
nvc_cot_dataset_df = pd.read_csv(csv_file_path)
nvc_cot_dataset = Dataset.from_pandas(nvc_cot_dataset_df)


def format_nvc_cot(example):
    """Formats the NVC CoT example for SFTTrainer."""
    instruction = example['instruction']
    chain_of_thought = example['chain_of_thought']
    return f"""### User Input:
{instruction}

### Chain of Thought:
{chain_of_thought}""" # We combine instruction and CoT into a single text for SFT


# --- 3. Configure Training Arguments ---
training_args = TrainingArguments(
    output_dir="./nvc_deepseek_cot_finetune", # Output directory for checkpoints
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4, # Adjust based on your GPU and desired batch size
    optim="paged_adamw_32bit", # Optimizer, you can experiment with different ones
    learning_rate=1e-5, # Learning rate, fine-tune this parameter
    lr_scheduler_type="cosine",
    save_strategy="steps",
    save_steps=500, # Save checkpoint every 500 steps
    logging_steps=100,
    num_train_epochs=3, # Number of training epochs, adjust as needed
    max_steps=-1, # Override num_train_epochs if you want to train for a fixed number of steps
    fp16=not torch.cuda.is_bf16_supported(), # Use fp16 if bf16 is not supported
    bf16=torch.cuda.is_bf16_supported(), # Use bf16 if supported for faster training on compatible GPUs
    dataloader_num_workers=0,
    gradient_checkpointing=True,
    push_to_hub=False, # Set to True and configure hub_model_id to push to Hugging Face Hub
    # hub_model_id="your_hf_username/your_nvc_chatbot_model", # Replace with your HF Hub model ID
)

# --- 4. Initialize SFTTrainer ---
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=nvc_cot_dataset,
    formatting_func=format_nvc_cot, # Formatting function for dataset
    max_seq_length=max_seq_length,
    args=training_args,
)

# --- 5. Start Fine-Tuning ---
trainer.train()

# --- 6. Save LoRA Adapter ---
model.save_lora("./nvc_deepseek_cot_lora") # Save LoRA adapter after fine-tuning
print("NVC DeepSeek CoT LoRA adapter saved to './nvc_deepseek_cot_lora'")

# --- 7. Example Inference (Optional) ---
# Load the LoRA adapter for inference (you can do this in a separate script)
if False: # Set to True to run inference example
    infer_model, infer_tokenizer = FastLanguageModel.from_pretrained(
        model_name="deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        max_seq_length=max_seq_length,
        load_in_4bit=True,
        dtype=None,
        max_lora_rank=lora_rank,
        gpu_memory_utilization=0.9,
    )
    infer_model = FastLanguageModel.get_peft_model(infer_model)
    infer_model = infer_model.load_lora("./nvc_deepseek_cot_lora") # Load the saved LoRA

    user_input_text = "I'm feeling really frustrated with my roommate, he never cleans up!"
    prompt = f"""### User Input:
{user_input_text}

### Chain of Thought:
"""

    inputs = tokenizer([prompt], return_tensors="pt").to(infer_model.device)
    outputs = infer_model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.7,
        top_p=0.9,
    )
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    cot_response = response.replace(prompt, "").strip() # Clean up response

    print(f"User Input: {user_input_text}")
    print(f"NVC Chain of Thought Response: {cot_response}")

print("NVC DeepSeek CoT Fine-tuning Completed.")
