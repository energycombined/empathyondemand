# NVC_DeepSeek_R1_Distill_Llama_8B_FINETUNE.py
# -*- coding: utf-8 -*-
"""
This script fine-tunes the DeepSeek model for chain-of-thought (CoT) reasoning
using Nonviolent Communication (NVC) principles, using a dataset generated by NVC_DeepSeek_R1_Distill_Llama_8B_DATA.py.
It leverages Unsloth and the Supervised Fine-Tuning (SFT) Trainer from the TRL library.
"""

from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset, Dataset
import pandas as pd
import torch

# --- 1. Load Model with LoRA Configuration ---
max_seq_length = 2048
lora_rank = 32  # LoRA rank, adjust as needed

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    max_seq_length=max_seq_length,
    load_in_4bit=True, # Keep লোড_ইন_4bit for memory efficiency
    dtype=None, # Auto detect dtype
    max_lora_rank=lora_rank,
    gpu_memory_utilization=0.9, # Adjust based on your GPU memory
)

model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,  # LoRA rank
    target_modules=["q_proj", "v_proj"],  # Target attention layers, you can expand this
    lora_alpha=lora_rank,
    lora_dropout=0.1,
    bias="none",
    use_gradient_checkpointing=True,
    random_state=42,
)

# --- 2. Load and Format the NVC CoT Dataset ---
csv_file_path = "nvc_cot_dataset.csv"  # Path to your generated dataset
nvc_cot_dataset_df = pd.read_csv(csv_file_path)
nvc_cot_dataset = Dataset.from_pandas(nvc_cot_dataset_df)


def format_nvc_cot(example):
    """Formats the NVC CoT example for SFTTrainer."""
    instruction = example['instruction']
    chain_of_thought = example['chain_of_thought']
    return f"""### User Input:
{instruction}

### Chain of Thought:
{chain_of_thought}""" # We combine instruction and CoT into a single text for SFT


# --- 3. Configure Training Arguments ---
training_args = TrainingArguments(
    output_dir="./nvc_deepseek_cot_finetune", # Output directory for checkpoints
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4, # Adjust based on your GPU and desired batch size
    optim="paged_adamw_32bit", # Optimizer, you can experiment with different ones
    learning_rate=1e-5, # Learning rate, fine-tune this parameter
    lr_scheduler_type="cosine",
    save_strategy="steps",
    save_steps=500, # Save checkpoint every 500 steps
    logging_steps=100,
    num_train_epochs=3, # Number of training epochs, adjust as needed
    max_steps=-1, # Override num_train_epochs if you want to train for a fixed number of steps
    fp16=not torch.cuda.is_bf16_supported(), # Use fp16 if bf16 is not supported
    bf16=torch.cuda.is_bf16_supported(), # Use bf16 if supported for faster training on compatible GPUs
    dataloader_num_workers=0,
    gradient_checkpointing=True,
    push_to_hub=False, # Set to True and configure hub_model_id to push to Hugging Face Hub
    # hub_model_id="your_hf_username/your_nvc_chatbot_model", # Replace with your HF Hub model ID
)

# --- 4. Initialize SFTTrainer ---
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=nvc_cot_dataset,
    formatting_func=format_nvc_cot, # Formatting function for dataset
    max_seq_length=max_seq_length,
    args=training_args,
)

# --- 5. Start Fine-Tuning ---
trainer.train()

# --- 6. Save LoRA Adapter ---
model.save_lora("./nvc_deepseek_cot_lora") # Save LoRA adapter after fine-tuning
print("NVC DeepSeek CoT LoRA adapter saved to './nvc_deepseek_cot_lora'")

# --- 7. Example Inference (Optional) ---
# Load the LoRA adapter for inference (you can do this in a separate script)
if False: # Set to True to run inference example
    infer_model, infer_tokenizer = FastLanguageModel.from_pretrained(
        model_name="deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        max_seq_length=max_seq_length,
        load_in_4bit=True,
        dtype=None,
        max_lora_rank=lora_rank,
        gpu_memory_utilization=0.9,
    )
    infer_model = FastLanguageModel.get_peft_model(infer_model)
    infer_model = infer_model.load_lora("./nvc_deepseek_cot_lora") # Load the saved LoRA

    user_input_text = "I'm feeling really frustrated with my roommate, he never cleans up!"
    prompt = f"""### User Input:
{user_input_text}

### Chain of Thought:
"""

    inputs = tokenizer([prompt], return_tensors="pt").to(infer_model.device)
    outputs = infer_model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.7,
        top_p=0.9,
    )
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    cot_response = response.replace(prompt, "").strip() # Clean up response

    print(f"User Input: {user_input_text}")
    print(f"NVC Chain of Thought Response: {cot_response}")

print("NVC DeepSeek CoT Fine-tuning Completed.")

# %% [markdown]
# Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!

# %% [markdown]
# <a name="Save"></a>
# ### Saving to float16 for VLLM
#
# We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.

# %%
# Merge to 16bit
if False: model.save_pretrained_merged("nvc_deepseek_cot_merged_16bit", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/your_username/nvc_deepseek_cot_merged_16bit", tokenizer, save_method = "merged_16bit", token = "") # Replace "hf/your_username/..." with your actual HF Hub path

# Merge to 4bit
if False: model.save_pretrained_merged("nvc_deepseek_cot_merged_4bit", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/your_username/nvc_deepseek_cot_merged_4bit", tokenizer, save_method = "merged_4bit", token = "") # Replace "hf/your_username/..." with your actual HF Hub path

# Just LoRA adapters - already saved above with model.save_lora("./nvc_deepseek_cot_lora")
# Example to resave LoRA
if False: model.save_pretrained_merged("nvc_deepseek_cot_lora_adapters", tokenizer, save_method = "lora",)
if False: model.push_to_hub_merged("hf/your_username/nvc_deepseek_cot_lora_adapters", tokenizer, save_method = "lora", token = "") # Replace "hf/your_username/..." with your actual HF Hub path

# %% [markdown]
# ### GGUF / llama.cpp Conversion
# To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.
#
# Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
# * `q8_0` - Fast conversion. High resource use, but generally acceptable.
# * `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.
# * `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.
#
# [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)

# %%
# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("nvc_deepseek_cot_gguf_q8_0", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("hf/your_username/nvc_deepseek_cot_gguf_q8_0", tokenizer, token = "") # Replace "hf/your_username/..." with your actual HF Hub path

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("nvc_deepseek_cot_gguf_f16", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/your_username/nvc_deepseek_cot_gguf_f16", tokenizer, quantization_method = "f16", token = "") # Replace "hf/your_username/..." with your actual HF Hub path

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("nvc_deepseek_cot_gguf_q4_k_m", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/your_username/nvc_deepseek_cot_gguf_q4_k_m", tokenizer, quantization_method = "q4_k_m", token = "") # Replace "hf/your_username/..." with your actual HF Hub path

# Save to multiple GGUF options - much faster if you want multiple!
if False:
    model.push_to_hub_gguf(
        "hf/your_username/nvc_deepseek_cot_gguf_multiple", # Replace "hf/your_username/..." with your actual HF Hub path
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "",
    )
